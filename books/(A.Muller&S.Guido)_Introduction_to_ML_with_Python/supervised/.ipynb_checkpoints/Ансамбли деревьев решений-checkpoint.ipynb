{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e681a19",
   "metadata": {},
   "source": [
    "# Ансамбли деревьев решений\n",
    "*Ensembles* - это методы, которые сочетают в себе множество моделей машинного обучения, чтобы в итоге дать ещё более мощную модель.\n",
    "\n",
    "Выделяют две основные ансамблевые модели\n",
    "## Случайный лес\n",
    "Как упоминалось ранее, деревья склонны к переобучению. Случайный лес призван решить эту проблему. Он представляет собой набор деревьев решений, где каждое дерево немного отличается от других. Причем каждое дерево может довольного хорошо прогнозировать, но **переобучаются на некоторой части данных**. Используя много таких деревьев, мы уменьшаем риск переобучиться путем усреднения результатов обучения всех деревьев. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb95659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6dd34",
   "metadata": {},
   "source": [
    "Параметр `n_estimators` характеризует количество деревьев, входящих в лес. Эти деревья будут построены совершенно независимо друг от друга, и алгоритм будет случайным образом отбирать признаки для построения каждого дерева.\n",
    "\n",
    "**bootstrap sample** - бутстреп-выборка - выборка того же размера, что и исходная, но в ней некоторые примеры будут отсутствовать, а некоторые попадут несколько раз. На основе сформированной bootstrap sample строится дерево решений следующим образом: для очередного разбиения узла случайным образом выбирается подмножество признаков, затем находится наилучший тест для этого подмножества. Мощность отбираемого подмножество регулируется параметром `max_features`.\n",
    "\n",
    "bootstrap sample + отбираемое подмножество признаков вместе приводят к тому, что все деревья в случайном лесе отличаются друг от друга, т.к. они обучаются на немного разных наборах данных и используют для обучения различные (относительно) признаки.\n",
    "\n",
    "*Замечание*: высокое значение `max_features` означает, что деревья в лесу легко аппроксимируют данные и мало чем отличаются; низкое значение `max_features` означает, что деревья сильно отличаются и, возможно, каждое имеет большую глубину, чтобы хорошо соответствовать данным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "db7f451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего признаков в наборе: 30\n",
      "Количество деревьев в лесу: 10\n",
      "Мощность отбираемого подмножества: auto\n",
      "Правильность на тестовом наборе: 0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data,\n",
    "                                                    cancer.target,\n",
    "                                                    random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Всего признаков в наборе: {cancer.data.shape[1]}')\n",
    "print(f'Количество деревьев в лесу: {model.n_estimators}')\n",
    "print(f'Мощность отбираемого подмножества: {model.max_features}')\n",
    "print(f'Правильность на тестовом наборе: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a801d3e2",
   "metadata": {},
   "source": [
    "Чтобы дать прогноз для случайного леса, алгоритм сначала делает прогноз для каждого дерева. Для регрессии результаты целевой переменной могут быть усреднены, а для классификации: каждое дерево вычисляет вероятности для принадлежности к каждому из классов, а итоговый класс получается путем усреднения наблюдений по всем деревьям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19af0825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего признаков в наборе: 30\n",
      "Количество деревьев в лесу: 55\n",
      "Мощность отбираемого подмножества: 20\n",
      "Правильность на тестовом наборе: 0.986013986013986\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=55, max_features=20, random_state=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Всего признаков в наборе: {cancer.data.shape[1]}')\n",
    "print(f'Количество деревьев в лесу: {model.n_estimators}')\n",
    "print(f'Мощность отбираемого подмножества: {model.max_features}')\n",
    "print(f'Правильность на тестовом наборе: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8765a94",
   "metadata": {},
   "source": [
    "### Резюме\n",
    "**Случайный лес** - одна из самых широко используемых моделей, поскольку она часто дает хорошее качество без утомительной настройки параметров и не требует нормализации данных.\n",
    "По сути случайный лес обладает всеми преимуществами деревьев, хотя не лишен недостатков:  \n",
    "- трудно интерпретировать результат для неспециалистов  \n",
    "- установка показателя `random_state` может значительно влиять на построение модели, поскольку случайный лес в построении использует генератор псевдослучайных чисел  \n",
    "- плохо работает на данных высокой размерности (например, на текстовых).\n",
    "Случайный лес требует много памяти, медленно обучается, поэтому, если требуется сохранение памяти и скорости, лучше рассмотреть другую модель (например, линейную).\n",
    "\n",
    "Обычно чем больше  число деревьев в лесу (`n_estimators`), тем лучше. Но обратная сторона увеличения деревьев в том, что с ростом количества деревьев требуется больше памяти и больше времени для обучения. Параметр `n_jobs` устанавливает количество ядер, которые используются при построении (они приводят к линейному росту: 2 ядра -> в 2 раза быстрее). Чтобы использовать все ядра компьютера, нужно установить `n_jobs=-1`.\n",
    "\n",
    "## Градиентный бустинг деревьев регрессии\n",
    "Ещё один ансамблевый метод, который объединяет в себе множество деревьев для создания более мощной модели. Эту модель (несмотря на название) можно использовать как для регрессии, так и для классификации.\n",
    "В градиентном бустинге строится последовательность деревьев, в которой каждое дерево пытается \"исправить\" ошибки предыдущего. По умолчанию в градиентном бустинге отсутствует случайность, вместо этого используется предварительная обрезка. Обычно деревья градиентного бустинга небольшие по глубине, что делает модель легче с точки зрения памяти и скорости вычислений.\n",
    "Идея градиентного бустинга - объединение простых (по отдельности слабых - weak learners) деревьев. Каждое дерево дает хорошие прогнозы только для части данных. Поэтому для улучшения нужно \"добавить\" несколько деревьев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f2a7c385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Правильности градиентного бустинга на тестовой выборке: 0.986013986013986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(n_estimators=75,\n",
    "                                   learning_rate=0.45,\n",
    "                                   random_state=15,\n",
    "                                  max_depth=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Правильности градиентного бустинга на тестовой выборке: {model.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247331e",
   "metadata": {},
   "source": [
    " `learning_rate` контролирует насколько сильно каждое дерево будет пытаться исправить ошибки предыдущего: высокое значение говорит о том, что каждое дерево вносит большие корректировки (то есть модель становится более сложной)  \n",
    " `max_depth` контролирует глубину деревьев, входящих в бустинг\n",
    " \n",
    " ### Резюме\n",
    " Градиентный бустинг также является одним из самых мощных и широко используемых приемов. Его основной недостаток в том, что он требует тщательной настройки параметров, но зато он обучается без необходимости нормализации и показывает отличные результаты. Его важной составляющие является соотношение параметров `n_estimators` и `learning_rate`. Градиентный бустинг, как и другие модели на деревьях, не очень хорошо для сильно разреженных данных."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
